{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Classifiers\n",
    "\n",
    "### 0-Rule (0-R)\n",
    "Just choose whichever class is most common always\n",
    "\n",
    "### 1-Rule (1-R)\n",
    "-Want to choose a single attribute to make determinations<br>\n",
    "Pseudo code:<br>\n",
    "For each attribute<br>\n",
    "&emsp;for each value of the attribute<br>\n",
    "&emsp;&emsp;dominant class $\\leftarrow$ determine most common class<br>\n",
    "&emsp;&emsp;predict dominant class for that value<br>\n",
    "&emsp;end<br>\n",
    "end<br>\n",
    "Select attribute with lowest error rate\n",
    "\n",
    "### Naive Bayes\n",
    "Generative model - looks at joint probability $P(X,Y)$ not conditional probability $P(X|Y)$<br>\n",
    "$P(C_k|X) = \\frac{P(C_k)P(X_1|C_k)P(X_2|C_k)...P(X_n|C_k)} {P(X)}$<br>\n",
    "Note that $P(X)$ is the same for every vector $X$ therefore can ignore denominator, and maximise for $C_k$<br>\n",
    "Note that smoothing can be necessary either epsilon or laplace<br>\n",
    "Works on nominal values\n",
    "\n",
    "### Logistic Regression\n",
    "Discriminative model - looks at conditional probability $P(X|Y)$ not joint probability $P(X,Y)$<br> \n",
    "Works on quantitative values<br>\n",
    "log odds: $log \\left(\\frac{P(x)}{1 - P(x)}\\right) = \\Theta \\cdot X$ <br>\n",
    "$P(x)/\\left(1 - P(x)\\right) = e^{\\Theta \\cdot X}$ <br>\n",
    "$P(x) = \\sigma(\\Theta ^T X) = \\frac{1}{1 + e^{-\\Theta \\cdot X}}$ <br>\n",
    "$\\theta^{\\text{(new)}}_{j} \\leftarrow \\theta^{\\text{(old)}}_{j} - \\eta \\frac{\\partial log L(\\theta)}{\\partial \\theta_j}$ <br>\n",
    "$\\theta^{\\text{(new)}}_{j} \\leftarrow \\theta^{\\text{(old)}}_{j} - \\eta \\sum_i \\left( \\sigma(\\theta x) - y^i \\right) x^i_j$ <br>\n",
    "softmax function\n",
    "\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
      "Accuracy = 0.5833\n",
      "\n",
      "Error Rate = (FP + FN) / (TP + FP + TN + FN)\n",
      "Error Rate = 0.4167\n",
      "\n",
      "Precision = (TP) / (TP + FP)\n",
      "Precision = 0.3750\n",
      "\n",
      "Recall = (TP) / (TP + FN)\n",
      "Recall = 0.3750\n",
      "\n",
      "F-score\n",
      "F-beta = [(1 + beta^2) prec * rec] /[(beta^2 * prec) + rec]\n",
      "F-1 = 0.281250 / 0.750000\n",
      "F-1 = 0.375000\n",
      "\n",
      "\n",
      "Macro Averaging run over each interesting class and average results\n",
      "Micro Averaging sum TP, FP, TN, FN before running\n"
     ]
    }
   ],
   "source": [
    "#Evaluation 1\n",
    "\n",
    "TP = 6\n",
    "FP = 10\n",
    "TN = 22\n",
    "FN = 10\n",
    "beta = 1\n",
    "\n",
    "print(\"Accuracy = (TP + TN) / (TP + FP + TN + FN)\")\n",
    "print(\"Accuracy = %0.4f\" %((TP+TN)/(TP+FP+TN+FN))   )\n",
    "\n",
    "print(\"\\nError Rate = (FP + FN) / (TP + FP + TN + FN)\")\n",
    "print(\"Error Rate = %0.4f\" %((FP+FN)/(TP+FP+TN+FN))   )\n",
    "\n",
    "prec = (TP)/(TP+FP)\n",
    "print(\"\\nPrecision = (TP) / (TP + FP)\")\n",
    "print(\"Precision = %0.4f\" %(prec)   )\n",
    "\n",
    "rec = (TP)/(TP+FN)\n",
    "print(\"\\nRecall = (TP) / (TP + FN)\")\n",
    "print(\"Recall = %0.4f\" %rec )\n",
    "\n",
    "print(\"\\nF-score\")\n",
    "print(\"F-beta = [(1 + beta^2) prec * rec] /[(beta^2 * prec) + rec]\")\n",
    "print(\"F-%d = %f / %f\" %(beta, (1 + beta*beta)*prec*rec, (beta*beta*prec) + rec ) )\n",
    "print(\"F-%d = %f\" %(beta, ((1 + beta*beta)*prec*rec) / ((beta*beta*prec) + rec )) )\n",
    "\n",
    "print(\"\\n\\nMacro Averaging run over each interesting class and average results\")\n",
    "print(\"Micro Averaging sum TP, FP, TN, FN before running\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "### ID3 Algorithm\n",
    "&emsp;(1) Select new attribute<br>\n",
    "&emsp;(2) Create branch for each value of said attribute<br>\n",
    "&emsp;(3) If any attributes have mixed labels and attributes remain goto (1)<br>\n",
    "\n",
    "### Information Gain\n",
    "Entropy:<br>\n",
    "$H(x) = -\\sum_i P(i)log_2(P(i))$<br>\n",
    "if $P(i)=0$ then let $P(i)log_2(P(i)) = 0$ <br>\n",
    "Information gain is the reduction in entropy $IG(R_A|R) = H(R) - MeanInfo(X)$<br>\n",
    "Where the meaninfo is the weighted entropies: $MeanInfo(X) = \\sum_i w(x_i)H(x_i)$<br>\n",
    "Information gain favours highly branching attributes which can lead to overfitting problems. Solution - Gain Ratio\n",
    "\n",
    "### Gain Ratio\n",
    "Gain Ratio is the ration of information gain to Split Info<br>\n",
    "$GR(R_A|R) = \\frac{IG(R_A|R)}{SI(R_A|R)}$ <br>\n",
    "$SI(R_A|R) = - \\sum_i w(x_i) log_2(w(x_i))$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "### Wrapper Methods\n",
    "Wrapper methods train the model on various combinations of features to see which one works best<br>\n",
    "Options:<br>\n",
    "    -Exhaustive Search<br>\n",
    "    -Greedy Search<br>\n",
    "    -Ablative Search<br>\n",
    "\n",
    "### Embedded Methods\n",
    "Work as a part of the algorithm i.e. decision trees or regularisation in regression<br>\n",
    "\n",
    "### Filter Methods\n",
    "Apply some value to features and select those with value above some cutoff<br>\n",
    "#### Mutual Information\n",
    "$PMI(X=i,Y=j) = log_2\\left( \\frac{P(X=i,Y=j)}{P(X=i)P(Y=j)} \\right)$<br><br>\n",
    "$MI(X;Y) = \\sum_{i \\in X} \\sum_{j \\in Y} W_{i,j} PMI(X=i,Y=j)$\n",
    "#### Chi-Square\n",
    "Difference between observered and expected joint distributions<br>\n",
    "$O(X) \\rightarrow $ Number of observations of X <br>\n",
    "$E(X=i,Y=j) = \\frac{O(X=i)O(Y=j)} {N}$ <br>\n",
    "$\\chi^2 = \\sum_{i \\in X} \\sum_{j \\in Y} \\frac{\\left( E(X=i,Y=j) - O(X=i,Y=j)\\right)^2}{E(X=i,Y=j)}$ <br>\n",
    "Can get p-value from $\\chi^2$ and degrees of freedom $dof = (labels-1)(features-1)$<br>\n",
    "Calculation not specified, if $p>= 0.05$ then independant, otherwise dependant<br>\n",
    "\n",
    "### Hybrids\n",
    "Reduce number of features then use wrapper methods to find best combination.\n",
    "\n",
    "### Projection and Principle Component Analysis\n",
    "Whereas feature selection involves removing unhelpful features, projection reduces the dimensionality of features<br>\n",
    "Transform data such that covariance matrix off diagonal elements are zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation II\n",
    "\n",
    "### Generalisation Error\n",
    "Generalisation Error = Bias squared + Variance + irreducible error\n",
    "\n",
    "### Bias\n",
    "Underfitting $\\rightarrow$ Low Bias<br>\n",
    "Overfitting $\\rightarrow$ High Bias\n",
    "\n",
    "### Variance\n",
    "Underfitting $\\rightarrow$ High Variance<br>\n",
    "Overfitting $\\rightarrow$ Low Variance\n",
    "\n",
    "### Learning Curve\n",
    "Learning Curve can demonstrate under or overfitting with increasing training set size. Overfitting will lead to low training error and high testing error.\n",
    "\n",
    "### Remedies\n",
    "#### Underfitting\n",
    "Boosting ensembling helps\n",
    "#### Overfitting\n",
    "Want more samples<br>\n",
    "Bagging (simulates more samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN\n",
    "\n",
    "### Metrics\n",
    "&emsp;-Euclidian Distance<br>\n",
    "&emsp;-Manhatten Distance<br>\n",
    "&emsp;-Cosine similarity $\\frac{a\\cdot b}{|a||b|}$<br>\n",
    "\n",
    "### Selection\n",
    "&emsp;-Majority Voting<br>\n",
    "&emsp;-Inverse Linear Distance<br>\n",
    "&emsp;&emsp;Rank order the distances (increasing) and calculate weightings as $w_j = \\frac{d_k - d_j}{d_k - d_1}$<br>\n",
    "&emsp;-Linear Distance<br>\n",
    "&emsp;&emsp; Weighting $w_j = \\frac{1}{d_j + \\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "### The Hyperplane\n",
    "Create a hyperplane such that it splits the classes with the largest margin<br>\n",
    "$f(x) = W^TX + b$\n",
    "W is weighting vector, X feature vector, b is bias<br>\n",
    "If $f(x) < 0$ in one class $f(x) > 0$ in the other<br>\n",
    "Support vectors $x^-$, and $x^+$ are the closest datapoints from each class $f(x^-)=-1$, $f(x^+)=1$<br>\n",
    "weight vector normal to the hyperplane\n",
    "### Kernel Trick\n",
    "To use SVM to classify non-linearly seperable data use kernal trick.<br>\n",
    "Kernal trick transforms\n",
    "\n",
    "### Soft Margin\n",
    "Soft margin gives flexibility by adding slack penalty to punish incorrectly classified training data. Best trick for noisy data.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Combination\n",
    "### Stacking\n",
    "Combining different algorithms. Computationally expensive, but can combine different types of classifiers.<br>\n",
    "### Bagging \n",
    "Reduce the variance of the model. Bootstrap Aggregating\n",
    "### Random Forest\n",
    "Random forest fixes problem of overfitting for random trees. Collection of trees using random subsets of features with bootstrap samples of data.\n",
    "### Boosting\n",
    "Reduce the bias of the model. If missclassified in one model, increase weight so classifiers learn different parts of the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "### Feature Normalisation\n",
    "Min-Max normalisation:<br>\n",
    "$x' = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}$<br>\n",
    "Standardisation z-score:<br>\n",
    "$x' = \\frac{x - x_{\\text{mean}}}{\\sigma}$<br>\n",
    "### K-means Algorithm\n",
    "(0) Pick random points as cluster centres <br>\n",
    "(1) Assign Data points to closest centre (Euclidian distance)<br>\n",
    "(2) Assign cluster centres to the mean of its data points <br>\n",
    "(3) go to step 1 <br>\n",
    "repeat until no points change cluster<br>\n",
    "\n",
    "### Unsupervised Evaluation\n",
    "#### Calinski-Harabasz Index\n",
    "The larger the better<br>\n",
    "$CH = \\frac{B_{SSE}}{W_{SSE}}\\cdot \\frac{N-K}{K-1}$<br>\n",
    "$B_{SSE} = \\sum_{clusters}n_i(\\mu - \\mu_i)^2$<br>\n",
    "$W_{SSE} = \\sum_{clusters} \\sum_{x \\in cluster}  (x-\\mu_i)^2$\n",
    "$B_{SSE}$ - between cluster sum of square error<br>\n",
    "$W_{SSE}$ - within cluster sum of square error<br>\n",
    "$N$ - total number of points<br>\n",
    "$n_i$ - number of points in cluster i<br>\n",
    "$\\mu$ - mean of all data points<br>\n",
    "$\\mu_i$ - mean of cluster i<br>\n",
    "#### Davies-Bouldin Index\n",
    "Measures similarity between clusters, the smaller the better <br>\n",
    "$s_i$ - the average distance between each point in cluster i and its centroid<br>\n",
    "$d_{ij}$ - the distance between cluster centroids i and j<br>\n",
    "define $R_i = max_{i \\neq j} \\frac{s_i + s_j}{d_{ij}} $ <br>\n",
    "$DB = \\frac{1}{K} \\sum_i R_i$ <br>\n",
    "\n",
    "#### Silhouette Coefficient\n",
    "Measure of how close a point is to points in some other cluster<br>\n",
    "For some point $x_k$ in cluster i:<br>\n",
    "Calculate the mean distance from point i to the other points in that cluster<br>\n",
    "$a(k) = \\frac{1}{n_i - 1}\\sum_{x \\in C_i, x\\neq x_k}d(x_k,x)$<br>\n",
    "Calcuate the min mean distance from $x_k$ to all the points in another cluster<br>\n",
    "$b(k) = min_{j \\neq i} \\frac{1}{n_j}\\sum_{x\\in C_j} d(x_k,x)$<br>\n",
    "Silhouette coefficient of point $k$:<br>\n",
    "$s(k) = \\frac{b(k) - a(k)}{max[a(k),b(k)]}$<br>\n",
    "$s(k)$ close to 1 implies sample far away from neighbouring clusters, close to 0 indicates near decision boundary, close to -1 implies might have been assigned to the wrong cluster. \n",
    "### Supervised Evaluation\n",
    "#### Entropy\n",
    "for cluster i and class j<br>\n",
    "$p_{ij} = n_{ij}/n_i$<br>\n",
    "$H_i = - \\sum_j^{C_i} p_{ij}log_2 p_{ij}$<br>\n",
    "$H = \\sum_i \\frac{n_i}{n} H_i$\n",
    "#### Purity\n",
    "Given $p_{ij}$ from entropy<br>\n",
    "Calculate the purity in each cluster<br>\n",
    "$purity_i = max(p_{ij})$<br>\n",
    "Calculate the purity of all clusters<br>\n",
    "$purity = \\sum_i \\frac{n_i}{n} purity_i$<br>\n",
    "#### V-measure\n",
    "Entropy and purity don't consider completeness i.e. are all the members of a given class assigned to the same cluster<br>\n",
    "v-measure based on homogeneity criterion h, and completeness criterion c<br>\n",
    "$h = 1 - \\frac{H(C|K)}{H(C)}$<br>\n",
    "$c = 1 - \\frac{H(K|C)}{H(K)}$<br>\n",
    "$v = \\frac{(1 + \\beta)hc}{\\beta h + c}$<br>\n",
    "for cluster i and class j\n",
    "$p_j = n_j/n$<br>\n",
    "$p_i = n_i/n$<br>\n",
    "$p_ij = n_ij/n$<br>\n",
    "$H(C) = - \\sum_{j\\in classes} p_j log_2 p_j$<br>\n",
    "$H(K) = - \\sum_{i\\in clusters} p_i log_2 p_i$<br>\n",
    "Entropy of class distribution in cluster i:<br>\n",
    "$Hi = - \\sum_{j\\in classes} p_{ij} log_2 p_{ij}$<br>\n",
    "Entropy of cluster distribution:<br>\n",
    "$Hj = - \\sum_{i\\in clusters} p_{ij} log_2 p_{ij}$<br>\n",
    "$H(C|K) = \\sum_i \\frac{n_i}{n}H_i$<br>\n",
    "$H(K|C) = \\sum_j \\frac{n_j}{n}H_j$<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4237949406953986"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xlog_2x(x):\n",
    "    if x < 0:\n",
    "        print(\"Error input to log_2 less than 0\")\n",
    "        return -1;\n",
    "    if x == 0: return 0\n",
    "    return (x*math.log2(x))\n",
    "\n",
    "def sig(x):\n",
    "    return 1/(1 + math.exp(-x))\n",
    "\n",
    "def dsig(x):\n",
    "    return sig(x)*(1 - sig(x))\n",
    "\n",
    "xlog_2x(5/8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
